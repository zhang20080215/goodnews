import feedparser
import requests
import re
import os
# ç¡®ä¿è¿™ä¸€è¡Œå¯¼å…¥äº†é…ç½®
from config import NEWS_SOURCES, FETCH_LIMIT_PER_SOURCE, HISTORY_FILE

class Scraper:
    def __init__(self):
        # ğŸ‘‡ ç¡®ä¿ä¸‹é¢è¿™äº›è¡Œå‰é¢æœ‰ 8 ä¸ªç©ºæ ¼ï¼ˆæˆ– 2 ä¸ª Tabï¼‰
        self.sources = NEWS_SOURCES
        self.limit = FETCH_LIMIT_PER_SOURCE
        self.history_file = HISTORY_FILE
        self.processed_urls = self._load_history()

    def _load_history(self):
        """åŠ è½½å·²å¤„ç†çš„ URL"""
        if os.path.exists(self.history_file):
            with open(self.history_file, 'r', encoding='utf-8') as f:
                return set(line.strip() for line in f if line.strip())
        return set()

    # ... å…¶ä»–å‡½æ•°ä¹Ÿè¦ä¿æŒæ­£ç¡®çš„ç¼©è¿›å±‚çº§

    def fetch_all(self, limit=3):
        news_list = []
        headers = {'User-Agent': 'Mozilla/5.0'}
        for url in self.sources:
            try:
                print(f"ğŸ” Scraping: {url}")
                resp = requests.get(url, headers=headers, timeout=15)
                if resp.status_code == 200:
                    feed = feedparser.parse(resp.content)
                    for entry in feed.entries[:limit]:
                        news_list.append({
                            "title": entry.title,
                            "summary": entry.get("summary", ""),
                            "link": entry.link
                        })
            except Exception as e:
                print(f"âš ï¸ Failed to fetch {url}: {e}")
        return news_list