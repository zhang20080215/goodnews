import feedparser
import requests # å¼•å…¥ requests æ¥æ‰‹åŠ¨æ§åˆ¶è¯·æ±‚å¤´

class Scraper:
    def __init__(self):
        self.sources = {
            "en": ["https://www.goodnewsnetwork.org/category/news/feed/"],
            "zh": ["https://www.thepaper.cn/rss_pms.jsp"] 
        }

    def fetch_all(self, limit=3):
        news_list = []
        # ğŸ’¡ ä¼ªè£…æˆçœŸå®çš„æµè§ˆå™¨è®¿é—®
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        for lang, urls in self.sources.items():
            for url in urls:
                try:
                    print(f"ğŸ” å°è¯•æŠ“å–: {url}")
                    # ğŸ’¡ å…ˆç”¨ requests æŠ“å–å†…å®¹ï¼Œå†äº¤ç»™ feedparser è§£æ
                    response = requests.get(url, headers=headers, timeout=15)
                    
                    if response.status_code == 200:
                        feed = feedparser.parse(response.content)
                        print(f"ğŸ“Š æˆåŠŸï¼æŠ“å–åˆ° {len(feed.entries)} æ¡æ–°é—»")
                        
                        for entry in feed.entries[:limit]:
                            news_list.append({
                                "title": entry.title,
                                "summary": entry.get("summary", ""),
                                "link": entry.link,
                                "lang": lang
                            })
                    else:
                        print(f"âŒ æŠ“å–å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                except Exception as e:
                    print(f"âš ï¸ å‡ºé”™: {e}")
                    
        return news_list